
==== 分类：MES重构的问题

==== 原来的有些表数据量太大，1亿到3亿
方法：分表，按年、按月分拆分表，分成当前表和历史表

==== jdk版本是1.8，需要升级成jdk17
方法：
<properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
        <maven.compiler.source>17</maven.compiler.source>
        <maven.compiler.target>17</maven.compiler.target>
        <java.version>17</java.version>
        <!-- spring cloud alibaba 版本 -->
        <spring.cloud.alibaba.version>2022.0.0.0-RC1</spring.cloud.alibaba.version>
        <!-- spring cloud版本 -->
        <spring-cloud.version>2022.0.0</spring-cloud.version>
        <hutool.version>5.8.18</hutool.version>
        <pagehelper.version>5.3.2</pagehelper.version>
        <springboot-admin.version>3.0.2</springboot-admin.version>
        <mybatis-plus.version>3.5.3.1</mybatis-plus.version>
        <fastjson.version>1.2.83</fastjson.version>
        <maven-plugin-plugin.version>3.4</maven-plugin-plugin.version>
        <knife4j-openapi3.version>4.0.0</knife4j-openapi3.version>
        <mybatis.version>3.0.1</mybatis.version>
        <javax.version>4.0.1</javax.version>
        <commons-io.version>2.11.0</commons-io.version>
        <logstash.version>7.3</logstash.version>
</properties>

==== 原来不是springcloud alibaba项目
方法：引入公共包（alibaba、mybatis、swagger、logstash等等），去除多余springboot依赖，变成springcloud alibaba项目

==== binlog配置的内存不够大，复制失败，分表时有的表有两三亿数据。
方法：调大或关闭binlog日志。

==== 旧代码有50多g的缓存，用于报表查询
方法：根据业务改成统计表实现

==== 生产环境不能调试
方法：用wireshark等抓包和kibana看日志看传参等信息

==== 复盘 回顾 如何有效回顾，总结经验教训
方法：
找出可以提前做，提高效率，降低风险的地方
复盘，找出为什么出现这个问题
往后这种问题，靠我们自己能做啥

吐槽会，总结整个期间项目优缺点
    内部、外部、前端、后端、产品、测试、运维、加班、经费等等


==== 碰到上游数据导致不好测试的问题
方法：开发、产品、上游拉一个小群，专门解决这个问题

==== 难度低，最重要的是如何写文档呈现出来。docker转k8s时，出现某个服务每小时重启的情况。
排查原因：
	主要是启动脚本服务内存比例太小，由1/2改为3/4即可
	接口调用频繁，老机器频繁通知，保证业务正常情况下限流

限流方法：配置文件配置，uri、每N秒允许通过1次请求

限流信息使用caffeine，一种带有存储和移除策略的Map，由Guava改进而来的高性能本地缓存，将通过给定的配置，自动移除“不常用”的数据，以保持内存的合理占用

本地测试方法：写java程序测试，测试请求和响应数量，对比下限流是否成功

结果：日请求7千万降低到1千万这样。其他有的接口20w一分钟限流到7万一分钟这样

mysql主从，深圳主库 越南从库 主从同步失败
新增会同步但是修改不会，跟同步策略有关系

电商mall-多数据源下事务
@DSTransactional com.baomidou.dynamic.datasource.annotation

电商mall mq改订单为已发货，后面看，订单状态还是待发货
订单已发货后还有收到微信支付完成的消息，mq会改订单为待发货

mes系统，excel导出乱码
使用了@RequestBody导致

mes系统，多线程解析bom树形结构，自定义线程池工具类，callable
erp的bom结构 20分钟 到 3分钟

技术方案定型
选型前提：知道、搜集到相关需求，核心需求是否满足
选型：第一：优先参考头部厂商，第二：搜索相关文档 第三：完善度，体系，时效性，扩展性

小程序商城支付
==== 微信支付金额比实际收到金额少，原因是银行卡使用立减优惠，最终发现用错字段了，


==== 如何做决策
    让他们都描述好自个观点，列出优劣势。优劣势区分不出来则让他们更详细说观点，选整体逻辑更清晰、细节更明确的。但是做决策时要这样回复，前期先按aaa的做，但是后期要考虑bbb的，散会。

==== mes越南工厂停电重启恢复问题：
来电--重启nacos集群--无法通过控制台管理集群--网络问题--车间交换机断电--恢复电力，网络恢复--nacos重启失败--k8s node节点时间不对--同步时间--nacos还是无法恢复（运维是k8s+etcd部署，etcd有脏数据导致集群k8s有问题）--两路走，一部分运维人员恢复旧集群，一部分创建新集群--恢复旧k8s集群，服务恢复


==== redis集群，数据重复问题：
人资开发用的redis集群，所有数据重复出现三次，新增的key也是显示三次，删除一个，另外两个还在，并且刷新后能查询出来。
一会新增的key又正常了，但是历史数据还是三个。
有一个redis数据分片有问题

==== 大数据startRockets 同步mysql数据（同步10g数据进startRockets100秒左右），极速查询分析。存算分离架构。
==== flink
 用于数据恢复，mysql数据恢复到elasticsearch，方便快捷（ai标注系统断电后丢数据，用的此方法恢复）
 大数据，可支持各种存储库数据同步，主流数据库都支持，sqlserver、mysql、kafka等。整库同步目前版本只支持部分数据库。

==== 电商小程序，多数据源事务使用注解@DSTransactional，部分售后表修改成功但是订单表商品表修改失败，逻辑没问题，排查发现其中还使用了@Transactional注解，导致@DSTransactional失效。为何失效得跟代码了
==== 电商小程序，以前的代码，待支付4.10元支付后只收到4.09元：排查后原因是旧代码在中间处理时使用了double数据类型，接收和转bigdecimal都没问题，问题是调微信支付接口时需要进行json转换，double丢失了精度。double有几个值是近似值不能用于金额计算
==== 电商小程序：分布式事务实现方式是多数据源加@DSTransactional

==== ai标注项目，字段自动设置类型的问题
没有提前设置mapping字段类型，新索引的日期类型自动创建为了long，导致与旧索引对不上

==== 微信小程序h5商城问题排查--h5链接复制出来，可以看到调用接口的信息。其实就是内嵌一个网页

==== 导入错误，经常性问题：
1、读的哪个sheet？
2、表头放哪一行？
3、表格里面放的是不是值，还是只是公式？

==== 电商小程序问题
app嵌入h5，使用iframe拉起微信支付，部分安卓手机存在隐私安全问题（iframe中存在不安全的链接正在尝试进行导航，需要允许跳转）


==== 电商小程序问题
app嵌入h5，使用iframe拉起微信支付，部分安卓手机存在隐私安全问题（iframe中存在不安全的链接正在尝试进行导航，需要允许跳转）

==== ai标注系统 elasticSearch mysql minio盘 nas盘
mysql(shardingjdbc按时间分表）,es做搜索等业务
数据来源：
跑机时，机器大概每小时记录一万图片数据-- 机器拿预上传url，传到阿里云oss -- 系统定时从oss同步图片信息到系统 --图片元信息到es、mysql，图片存到minio网盘
机器本地存储的图片，手动放nas盘 -- 系统选中文件夹，同步到系统 --图片元信息到es、mysql，图片存到minio网盘

==== 电商小程序，售后总逻辑：抓资金流，钱没退回去则交易成功

==== iot用mqtt和netty？超过 2 百万了，肯定mqtt比netty好
netty做轻量级的

==== ai标注系统资源占用情况2024-8-22
CONTAINER ID     NAME            CPU %    MEM USAGE / LIMIT     MEM %      NET I/O             BLOCK I/O            PIDS
a959d9a47449     elasticsearch   0.23%    21.73GiB / 62.59GiB   34.71%     13.5GB / 10.2GB     620MB / 21.5kB       166
25ff057d74a6     minio           0.35%    1.981GiB / 62.59GiB   3.16%      624GB / 42.3GB      710MB / 0B           527
be0ced1ed974     mark-service    0.04%    7.042GiB / 62.59GiB   11.25%     38.3GB / 34.2GB     328MB / 0B           175
bd1cd1fef845     mysql_174       0.06%    1.89GiB / 62.59GiB    3.02%      19.4GB / 20.8GB     12GB / 320GB         100
7e08d6b1e0b9     redis           89.00%   15GiB / 62.59GiB      23.96%     2.94GB / 40.3GB     4.01GB / 6.66TB      5

==== 微服务拆分：
	一般根据业务分，用户权限、订单等
	导出功能，独立一个服务，避免影响正常
	数据统计功能，独立一个服务，避免影响正常

==== mysql优化：
搞多数据源，读写分离
单数据源：内存调整大点，写持久化的磁盘、写binlog等日志的磁盘分离，提升写入速度

==== es优化：
ai标注，同步图片特别慢（图片表就有2亿图片记录，还有其他小点的表），原因是es用的zookeeper注册中心，内存占用一直很大，调大内存就行

==== ai标注系统，记一次生产环境问题排查
ai标注系统，同步文件夹下图片时一部分图片同步不过来，但是没有异常日志

导出堆栈日志排查问题：
一、找pid：
正常 ps -ef | grep 应用名
发现是docker启动的程序，ss -tnlp | grep docker。然后看应用端口，拿pid（进docker容器，用正常linux命令拿堆栈日志）

二、导出堆栈日志 jstack pid > info.txt

三、看堆栈日志，
看线程是否blocked，判断死锁
看线程WAITING状态时，等待的是啥。典型的是网络和文件io一直等待
看应用包名，有木有在堆栈中输出

最终发现，org.apache.commons.io.IOUtils.toByteArray一直在等待中，配合代码，发现是下载阿里云图片阻塞了（深层次原因待分析），此情况临时重启解决用户问题

==== 根据资源占用图表，分析系统问题，ai标注每天定时任务同步图片，100多万，负载起来后马上下降了，两小时后又上去：
下降原因：猜测是某方面出现瓶颈了，minio、mysql，这些直接写磁盘的组件，可能性最大
根据此推测，看磁盘写入图表，这个时间磁盘写入一直是拉满的

==== ai标注系统，上传图片时不时卡顿，有一两张图片上传失败，或者整个文件夹上传失败：实现线程池一些方法，输出日志，监控执行状态
注：时间和资源允许的话，最终方案可以试试minioClient是单例，单线程多线程差别不大，最好单独整个图片上传服务，搞集群，每个节点都独立写入，速度更快


排查发现是minio限流了
原因：
    minio单文件夹超过100万，会卡，有定时任务一直往一个文件夹写文件，至少写100万
    定时任务与上传图片的任务，共用线程池，排队，导致很慢
临时解决，急用：
    重启minio和java，重新上传。定时任务未同步的，程序有处理，未同步的都提前写redis上了，同步一个干掉一个
解决过程1：
    1、定时任务单独启一个线程池，正常任务用另一个，不影响正常任务
    2、增加限流策略，考虑到全局限流，用redission的RRateLimiter
    3、minio写入接口重试，重试次数10次
    4、（不可行）使用分布式锁，保证一次只写入一个文件？不行只是写入某个文件夹卡，写其他的不卡，全局锁会导致写某个大文件时卡主，其他小文件写不进去

解决过程2：
    1、去除限流策略，移除RRateLimiter限流代码
    2、线程池的线程数量调成2，越少越好，每个任务写入2000条记录，线程多了则minio、mysql吃不消
效果：
    图片数量不再少，但定时任务耗时从8小时到了18小时，显然不能接受

解决过程3：
    调高线程个数2个调到10个，考虑到写入minio后会写入mysql,写完mysql此任务才完成，线程等待时间长，根据系统监控，磁盘的写入间隔，设置数量
效果：
    每秒存储22张图片，基本满足使用



==== 并发、限流与全局锁的理解：

并发类型：
多而快，根据处理速度，线程可以调多：并发查询，线程数量适当增加
多而慢，根据处理速度，线程数量要少：大批量文件写入，例如图片同步这一类。线程多了反而导致线程等待时间多，cpu占用高，系统卡顿


限流：
    针对多任务并行执行，限制速率
分布式锁：
    针对多任务拿同一个资源时
